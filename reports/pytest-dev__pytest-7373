[{"repo": "pytest-dev/pytest", "instance_id": "pytest-dev__pytest-7373", "base_commit": "7b77fc086aab8b3a8ebc890200371884555eea1e", "patch": "diff --git a/src/_pytest/mark/evaluate.py b/src/_pytest/mark/evaluate.py\n--- a/src/_pytest/mark/evaluate.py\n+++ b/src/_pytest/mark/evaluate.py\n@@ -10,25 +10,14 @@\n from ..outcomes import fail\n from ..outcomes import TEST_OUTCOME\n from .structures import Mark\n-from _pytest.config import Config\n from _pytest.nodes import Item\n-from _pytest.store import StoreKey\n \n \n-evalcache_key = StoreKey[Dict[str, Any]]()\n+def compiled_eval(expr: str, d: Dict[str, object]) -> Any:\n+    import _pytest._code\n \n-\n-def cached_eval(config: Config, expr: str, d: Dict[str, object]) -> Any:\n-    default = {}  # type: Dict[str, object]\n-    evalcache = config._store.setdefault(evalcache_key, default)\n-    try:\n-        return evalcache[expr]\n-    except KeyError:\n-        import _pytest._code\n-\n-        exprcode = _pytest._code.compile(expr, mode=\"eval\")\n-        evalcache[expr] = x = eval(exprcode, d)\n-        return x\n+    exprcode = _pytest._code.compile(expr, mode=\"eval\")\n+    return eval(exprcode, d)\n \n \n class MarkEvaluator:\n@@ -98,7 +87,7 @@ def _istrue(self) -> bool:\n                     self.expr = expr\n                     if isinstance(expr, str):\n                         d = self._getglobals()\n-                        result = cached_eval(self.item.config, expr, d)\n+                        result = compiled_eval(expr, d)\n                     else:\n                         if \"reason\" not in mark.kwargs:\n                             # XXX better be checked at collection time\n", "test_patch": "diff --git a/testing/test_mark.py b/testing/test_mark.py\n--- a/testing/test_mark.py\n+++ b/testing/test_mark.py\n@@ -706,6 +706,36 @@ def test_1(parameter):\n         reprec = testdir.inline_run()\n         reprec.assertoutcome(skipped=1)\n \n+    def test_reevaluate_dynamic_expr(self, testdir):\n+        \"\"\"#7360\"\"\"\n+        py_file1 = testdir.makepyfile(\n+            test_reevaluate_dynamic_expr1=\"\"\"\n+            import pytest\n+\n+            skip = True\n+\n+            @pytest.mark.skipif(\"skip\")\n+            def test_should_skip():\n+                assert True\n+        \"\"\"\n+        )\n+        py_file2 = testdir.makepyfile(\n+            test_reevaluate_dynamic_expr2=\"\"\"\n+            import pytest\n+\n+            skip = False\n+\n+            @pytest.mark.skipif(\"skip\")\n+            def test_should_not_skip():\n+                assert True\n+        \"\"\"\n+        )\n+\n+        file_name1 = os.path.basename(py_file1.strpath)\n+        file_name2 = os.path.basename(py_file2.strpath)\n+        reprec = testdir.inline_run(file_name1, file_name2)\n+        reprec.assertoutcome(passed=1, skipped=1)\n+\n \n class TestKeywordSelection:\n     def test_select_simple(self, testdir):\n", "problem_statement": "Incorrect caching of skipif/xfail string condition evaluation\nVersion: pytest 5.4.3, current master\r\n\r\npytest caches the evaluation of the string in e.g. `@pytest.mark.skipif(\"sys.platform == 'win32'\")`. The caching key is only the string itself (see `cached_eval` in `_pytest/mark/evaluate.py`). However, the evaluation also depends on the item's globals, so the caching can lead to incorrect results. Example:\r\n\r\n```py\r\n# test_module_1.py\r\nimport pytest\r\n\r\nskip = True\r\n\r\n@pytest.mark.skipif(\"skip\")\r\ndef test_should_skip():\r\n    assert False\r\n```\r\n\r\n```py\r\n# test_module_2.py\r\nimport pytest\r\n\r\nskip = False\r\n\r\n@pytest.mark.skipif(\"skip\")\r\ndef test_should_not_skip():\r\n    assert False\r\n```\r\n\r\nRunning `pytest test_module_1.py test_module_2.py`.\r\n\r\nExpected: `test_should_skip` is skipped, `test_should_not_skip` is not skipped.\r\n\r\nActual: both are skipped.\r\n\r\n---\r\n\r\nI think the most appropriate fix is to simply remove the caching, which I don't think is necessary really, and inline `cached_eval` into `MarkEvaluator._istrue`.\n", "hints_text": "> I think the most appropriate fix is to simply remove the caching, which I don't think is necessary really, and inline cached_eval into MarkEvaluator._istrue.\r\n\r\nI agree:\r\n\r\n* While it might have some performance impact with very large test suites which use marks with eval, the simple workaround is to not use the eval feature on those, which is more predictable anyway.\r\n* I don't see a clean way to turn \"globals\" in some kind of cache key without having some performance impact and/or adverse effects.\r\n\r\nSo \ud83d\udc4d from me to simply removing this caching. \nAs globals are dynamic, i would propose to drop the cache as well, we should investigate reinstating a cache later on ", "created_at": "2020-06-15T17:12:08Z", "version": "5.4", "FAIL_TO_PASS": "[\"testing/test_mark.py::TestFunctional::test_reevaluate_dynamic_expr\"]", "PASS_TO_PASS": "[\"testing/test_mark.py::TestMark::test_pytest_exists_in_namespace_all[py.test-mark]\", \"testing/test_mark.py::TestMark::test_pytest_exists_in_namespace_all[py.test-param]\", \"testing/test_mark.py::TestMark::test_pytest_exists_in_namespace_all[pytest-mark]\", \"testing/test_mark.py::TestMark::test_pytest_exists_in_namespace_all[pytest-param]\", \"testing/test_mark.py::TestMark::test_pytest_mark_notcallable\", \"testing/test_mark.py::TestMark::test_mark_with_param\", \"testing/test_mark.py::TestMark::test_pytest_mark_name_starts_with_underscore\", \"testing/test_mark.py::TestMarkDecorator::test__eq__[lhs0-rhs0-True]\", \"testing/test_mark.py::TestMarkDecorator::test__eq__[lhs1-rhs1-False]\", \"testing/test_mark.py::TestMarkDecorator::test__eq__[lhs2-bar-False]\", \"testing/test_mark.py::TestMarkDecorator::test__eq__[foo-rhs3-False]\", \"testing/test_mark.py::TestMarkDecorator::test_aliases\", \"testing/test_mark.py::test_addmarker_order\", \"testing/test_mark.py::test_pytest_param_id_requires_string\", \"testing/test_mark.py::test_pytest_param_id_allows_none_or_string[None]\", \"testing/test_mark.py::test_pytest_param_id_allows_none_or_string[hello\", \"testing/test_mark.py::test_marked_class_run_twice\", \"testing/test_mark.py::test_ini_markers\", \"testing/test_mark.py::test_markers_option\", \"testing/test_mark.py::test_ini_markers_whitespace\", \"testing/test_mark.py::test_marker_without_description\", \"testing/test_mark.py::test_markers_option_with_plugin_in_current_dir\", \"testing/test_mark.py::test_mark_on_pseudo_function\", \"testing/test_mark.py::test_strict_prohibits_unregistered_markers[--strict-markers]\", \"testing/test_mark.py::test_strict_prohibits_unregistered_markers[--strict]\", \"testing/test_mark.py::test_mark_option[xyz-expected_passed0]\", \"testing/test_mark.py::test_mark_option[(((\", \"testing/test_mark.py::test_mark_option[not\", \"testing/test_mark.py::test_mark_option[xyz\", \"testing/test_mark.py::test_mark_option[xyz2-expected_passed4]\", \"testing/test_mark.py::test_mark_option_custom[interface-expected_passed0]\", \"testing/test_mark.py::test_mark_option_custom[not\", \"testing/test_mark.py::test_keyword_option_custom[interface-expected_passed0]\", \"testing/test_mark.py::test_keyword_option_custom[not\", \"testing/test_mark.py::test_keyword_option_custom[pass-expected_passed2]\", \"testing/test_mark.py::test_keyword_option_custom[1\", \"testing/test_mark.py::test_keyword_option_considers_mark\", \"testing/test_mark.py::test_keyword_option_parametrize[None-expected_passed0]\", \"testing/test_mark.py::test_keyword_option_parametrize[[1.3]-expected_passed1]\", \"testing/test_mark.py::test_keyword_option_parametrize[2-3-expected_passed2]\", \"testing/test_mark.py::test_parametrize_with_module\", \"testing/test_mark.py::test_keyword_option_wrong_arguments[foo\", \"testing/test_mark.py::test_keyword_option_wrong_arguments[(foo-at\", \"testing/test_mark.py::test_keyword_option_wrong_arguments[or\", \"testing/test_mark.py::test_keyword_option_wrong_arguments[not\", \"testing/test_mark.py::test_parametrized_collected_from_command_line\", \"testing/test_mark.py::test_parametrized_collect_with_wrong_args\", \"testing/test_mark.py::test_parametrized_with_kwargs\", \"testing/test_mark.py::test_parametrize_iterator\", \"testing/test_mark.py::TestFunctional::test_merging_markers_deep\", \"testing/test_mark.py::TestFunctional::test_mark_decorator_subclass_does_not_propagate_to_base\", \"testing/test_mark.py::TestFunctional::test_mark_should_not_pass_to_siebling_class\", \"testing/test_mark.py::TestFunctional::test_mark_decorator_baseclasses_merged\", \"testing/test_mark.py::TestFunctional::test_mark_closest\", \"testing/test_mark.py::TestFunctional::test_mark_with_wrong_marker\", \"testing/test_mark.py::TestFunctional::test_mark_dynamically_in_funcarg\", \"testing/test_mark.py::TestFunctional::test_no_marker_match_on_unmarked_names\", \"testing/test_mark.py::TestFunctional::test_keywords_at_node_level\", \"testing/test_mark.py::TestFunctional::test_keyword_added_for_session\", \"testing/test_mark.py::TestFunctional::test_mark_from_parameters\", \"testing/test_mark.py::TestKeywordSelection::test_select_simple\", \"testing/test_mark.py::TestKeywordSelection::test_select_extra_keywords[xxx]\", \"testing/test_mark.py::TestKeywordSelection::test_select_extra_keywords[xxx\", \"testing/test_mark.py::TestKeywordSelection::test_select_extra_keywords[TestClass]\", \"testing/test_mark.py::TestKeywordSelection::test_select_extra_keywords[TestClass\", \"testing/test_mark.py::TestKeywordSelection::test_select_starton\", \"testing/test_mark.py::TestKeywordSelection::test_keyword_extra\", \"testing/test_mark.py::TestKeywordSelection::test_no_magic_values[__]\", \"testing/test_mark.py::TestKeywordSelection::test_no_magic_values[+]\", \"testing/test_mark.py::TestKeywordSelection::test_no_magic_values[..]\", \"testing/test_mark.py::TestKeywordSelection::test_no_match_directories_outside_the_suite\", \"testing/test_mark.py::test_parameterset_for_parametrize_marks[None]\", \"testing/test_mark.py::test_parameterset_for_parametrize_marks[]\", \"testing/test_mark.py::test_parameterset_for_parametrize_marks[skip]\", \"testing/test_mark.py::test_parameterset_for_parametrize_marks[xfail]\", \"testing/test_mark.py::test_parameterset_for_fail_at_collect\", \"testing/test_mark.py::test_parameterset_for_parametrize_bad_markname\", \"testing/test_mark.py::test_mark_expressions_no_smear\", \"testing/test_mark.py::test_markers_from_parametrize\", \"testing/test_mark.py::test_marker_expr_eval_failure_handling[NOT\", \"testing/test_mark.py::test_marker_expr_eval_failure_handling[bogus/]\"]", "environment_setup_commit": "678c1a0745f1cf175c442c719906a1f13e496910"}, "On branch main\nChanges not staged for commit:\n  (use \"git add <file>...\" to update what will be committed)\n  (use \"git restore <file>...\" to discard changes in working directory)\n\tmodified:   src/_pytest/mark/evaluate.py\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\ncommit 7b77fc086aab8b3a8ebc890200371884555eea1e\nMerge: 4f4c2638d bb878a2b1\nAuthor: Ran Benita <ran@unusedvar.com>\nDate:   Mon Jun 15 19:41:21 2020 +0300\n\n    Merge pull request #7368 from bluetech/teardown-in-setup\n    \n    runner: don't try to teardown previous items from pytest_runtest_setup\n\ndiff --git a/src/_pytest/mark/evaluate.py b/src/_pytest/mark/evaluate.py\nindex 759191668..eb9903a59 100644\n--- a/src/_pytest/mark/evaluate.py\n+++ b/src/_pytest/mark/evaluate.py\n@@ -10,25 +10,14 @@ from typing import Optional\n from ..outcomes import fail\n from ..outcomes import TEST_OUTCOME\n from .structures import Mark\n-from _pytest.config import Config\n from _pytest.nodes import Item\n-from _pytest.store import StoreKey\n \n \n-evalcache_key = StoreKey[Dict[str, Any]]()\n+def compiled_eval(expr: str, d: Dict[str, object]) -> Any:\n+    import _pytest._code\n \n-\n-def cached_eval(config: Config, expr: str, d: Dict[str, object]) -> Any:\n-    default = {}  # type: Dict[str, object]\n-    evalcache = config._store.setdefault(evalcache_key, default)\n-    try:\n-        return evalcache[expr]\n-    except KeyError:\n-        import _pytest._code\n-\n-        exprcode = _pytest._code.compile(expr, mode=\"eval\")\n-        evalcache[expr] = x = eval(exprcode, d)\n-        return x\n+    exprcode = _pytest._code.compile(expr, mode=\"eval\")\n+    return eval(exprcode, d)\n \n \n class MarkEvaluator:\n@@ -98,7 +87,7 @@ class MarkEvaluator:\n                     self.expr = expr\n                     if isinstance(expr, str):\n                         d = self._getglobals()\n-                        result = cached_eval(self.item.config, expr, d)\n+                        result = compiled_eval(expr, d)\n                     else:\n                         if \"reason\" not in mark.kwargs:\n                             # XXX better be checked at collection time\nObtaining file:///testbed\n  Installing build dependencies: started\n  Installing build dependencies: finished with status 'done'\n  Checking if build backend supports build_editable: started\n  Checking if build backend supports build_editable: finished with status 'done'\n  Getting requirements to build editable: started\n  Getting requirements to build editable: finished with status 'done'\n  Preparing editable metadata (pyproject.toml): started\n  Preparing editable metadata (pyproject.toml): finished with status 'done'\nRequirement already satisfied: attrs>=17.4.0 in /opt/miniconda3/envs/testbed/lib/python3.9/site-packages (from pytest==5.4.1.dev522+g7b77fc086.d20220101) (23.1.0)\nRequirement already satisfied: iniconfig in /opt/miniconda3/envs/testbed/lib/python3.9/site-packages (from pytest==5.4.1.dev522+g7b77fc086.d20220101) (2.0.0)\nRequirement already satisfied: more-itertools>=4.0.0 in /opt/miniconda3/envs/testbed/lib/python3.9/site-packages (from pytest==5.4.1.dev522+g7b77fc086.d20220101) (10.1.0)\nRequirement already satisfied: packaging in /opt/miniconda3/envs/testbed/lib/python3.9/site-packages (from pytest==5.4.1.dev522+g7b77fc086.d20220101) (23.1)\nRequirement already satisfied: pluggy<1.0,>=0.12 in /opt/miniconda3/envs/testbed/lib/python3.9/site-packages (from pytest==5.4.1.dev522+g7b77fc086.d20220101) (0.13.1)\nRequirement already satisfied: py>=1.5.0 in /opt/miniconda3/envs/testbed/lib/python3.9/site-packages (from pytest==5.4.1.dev522+g7b77fc086.d20220101) (1.11.0)\nRequirement already satisfied: toml in /opt/miniconda3/envs/testbed/lib/python3.9/site-packages (from pytest==5.4.1.dev522+g7b77fc086.d20220101) (0.10.2)\nBuilding wheels for collected packages: pytest\n  Building editable for pytest (pyproject.toml): started\n  Building editable for pytest (pyproject.toml): finished with status 'done'\n  Created wheel for pytest: filename=pytest-5.4.1.dev522+g7b77fc086.d20220101-0.editable-py3-none-any.whl size=5263 sha256=f53eacea499d959c039d71ded7e0f3584aab9e8eb74aee961976707d32544c81\n  Stored in directory: /tmp/pip-ephem-wheel-cache-kug1csgv/wheels/7d/66/67/70d1ee2124ccf21d601c352e25cdca10f611f7c8b3f9ffb9e4\nSuccessfully built pytest\nInstalling collected packages: pytest\n  Attempting uninstall: pytest\n    Found existing installation: pytest 5.4.1.dev522+g7b77fc086\n    Uninstalling pytest-5.4.1.dev522+g7b77fc086:\n      Successfully uninstalled pytest-5.4.1.dev522+g7b77fc086\nSuccessfully installed pytest-5.4.1.dev522+g7b77fc086.d20220101\n============================= test session starts ==============================\nplatform linux -- Python 3.9.20, pytest-5.4.1.dev522+g7b77fc086.d20220101, py-1.11.0, pluggy-0.13.1\nrootdir: /testbed, configfile: pyproject.toml\ncollected 92 items\n\ntesting/test_mark.py ................................................... [ 55%]\n.........................x...............                                [100%]\n\n==================================== PASSES ====================================\n_________________________ test_marked_class_run_twice __________________________\n----------------------------- Captured stdout call -----------------------------\n============================= test session starts ==============================\nplatform linux -- Python 3.9.20, pytest-5.4.1.dev522+g7b77fc086.d20220101, py-1.11.0, pluggy-0.13.1\nrootdir: /tmp/pytest-of-root/pytest-0/test_marked_class_run_twice0\ncollected 6 items\n\ntest_marked_class_run_twice.py ......\n\n============================== 6 passed in 0.01s ===============================\n_______________________________ test_ini_markers _______________________________\n----------------------------- Captured stdout call -----------------------------\n============================= test session starts ==============================\nplatform linux -- Python 3.9.20, pytest-5.4.1.dev522+g7b77fc086.d20220101, py-1.11.0, pluggy-0.13.1\nrootdir: /tmp/pytest-of-root/pytest-0/test_ini_markers0, configfile: tox.ini\ncollected 1 item\n\ntest_ini_markers.py .                                                    [100%]\n\n============================== 1 passed in 0.01s ===============================\n_____________________________ test_markers_option ______________________________\n----------------------------- Captured stdout call -----------------------------\n@pytest.mark.a1: this is a webtest marker\n\n@pytest.mark.a1some: another marker\n\n@pytest.mark.nodescription:\n\n@pytest.mark.filterwarnings(warning): add a warning filter to the given test. see https://docs.pytest.org/en/latest/warnings.html#pytest-mark-filterwarnings \n\n@pytest.mark.skip(reason=None): skip the given test function with an optional reason. Example: skip(reason=\"no way of currently testing this\") skips the test.\n\n@pytest.mark.skipif(condition): skip the given test function if eval(condition) results in a True value.  Evaluation happens within the module global context. Example: skipif('sys.platform == \"win32\"') skips the test if we are on the win32 platform. see https://docs.pytest.org/en/latest/skipping.html\n\n@pytest.mark.xfail(condition, reason=None, run=True, raises=None, strict=False): mark the test function as an expected failure if eval(condition) has a True value. Optionally specify a reason for better reporting and run=False if you don't even want to execute the test function. If only specific exception(s) are expected, you can list them in raises, and if the test fails in other ways, it will be reported as a true failure. See https://docs.pytest.org/en/latest/skipping.html\n\n@pytest.mark.parametrize(argnames, argvalues): call a test function multiple times passing in different arguments in turn. argvalues generally needs to be a list of values if argnames specifies only one name or a list of tuples of values if argnames specifies multiple names. Example: @parametrize('arg1', [1,2]) would lead to two calls of the decorated test function, one with arg1=1 and another with arg1=2.see https://docs.pytest.org/en/latest/parametrize.html for more info and examples.\n\n@pytest.mark.usefixtures(fixturename1, fixturename2, ...): mark tests as needing all of the specified fixtures. see https://docs.pytest.org/en/latest/fixture.html#usefixtures \n\n@pytest.mark.tryfirst: mark a hook implementation function such that the plugin machinery will try to call it first/as early as possible.\n\n@pytest.mark.trylast: mark a hook implementation function such that the plugin machinery will try to call it last/as late as possible.\n\n_________________________ test_ini_markers_whitespace __________________________\n----------------------------- Captured stdout call -----------------------------\n============================= test session starts ==============================\nplatform linux -- Python 3.9.20, pytest-5.4.1.dev522+g7b77fc086.d20220101, py-1.11.0, pluggy-0.13.1\nrootdir: /tmp/pytest-of-root/pytest-0/test_ini_markers_whitespace0, configfile: tox.ini\ncollected 1 item\n\ntest_ini_markers_whitespace.py .                                         [100%]\n\n============================== 1 passed in 0.01s ===============================\n_______________________ test_marker_without_description ________________________\n----------------------------- Captured stdout call -----------------------------\n============================= test session starts ==============================\nplatform linux -- Python 3.9.20, pytest-5.4.1.dev522+g7b77fc086.d20220101, py-1.11.0, pluggy-0.13.1\nrootdir: /tmp/pytest-of-root/pytest-0/test_marker_without_description0, configfile: setup.cfg\ncollected 0 items\n\n============================ no tests ran in 0.00s =============================\n________________ test_markers_option_with_plugin_in_current_dir ________________\n----------------------------- Captured stdout call -----------------------------\n@pytest.mark.flip:flop\n\n@pytest.mark.filterwarnings(warning): add a warning filter to the given test. see https://docs.pytest.org/en/latest/warnings.html#pytest-mark-filterwarnings \n\n@pytest.mark.skip(reason=None): skip the given test function with an optional reason. Example: skip(reason=\"no way of currently testing this\") skips the test.\n\n@pytest.mark.skipif(condition): skip the given test function if eval(condition) results in a True value.  Evaluation happens within the module global context. Example: skipif('sys.platform == \"win32\"') skips the test if we are on the win32 platform. see https://docs.pytest.org/en/latest/skipping.html\n\n@pytest.mark.xfail(condition, reason=None, run=True, raises=None, strict=False): mark the test function as an expected failure if eval(condition) has a True value. Optionally specify a reason for better reporting and run=False if you don't even want to execute the test function. If only specific exception(s) are expected, you can list them in raises, and if the test fails in other ways, it will be reported as a true failure. See https://docs.pytest.org/en/latest/skipping.html\n\n@pytest.mark.parametrize(argnames, argvalues): call a test function multiple times passing in different arguments in turn. argvalues generally needs to be a list of values if argnames specifies only one name or a list of tuples of values if argnames specifies multiple names. Example: @parametrize('arg1', [1,2]) would lead to two calls of the decorated test function, one with arg1=1 and another with arg1=2.see https://docs.pytest.org/en/latest/parametrize.html for more info and examples.\n\n@pytest.mark.usefixtures(fixturename1, fixturename2, ...): mark tests as needing all of the specified fixtures. see https://docs.pytest.org/en/latest/fixture.html#usefixtures \n\n@pytest.mark.tryfirst: mark a hook implementation function such that the plugin machinery will try to call it first/as early as possible.\n\n@pytest.mark.trylast: mark a hook implementation function such that the plugin machinery will try to call it last/as late as possible.\n\n_________________________ test_mark_on_pseudo_function _________________________\n----------------------------- Captured stdout call -----------------------------\n============================= test session starts ==============================\nplatform linux -- Python 3.9.20, pytest-5.4.1.dev522+g7b77fc086.d20220101, py-1.11.0, pluggy-0.13.1\nrootdir: /tmp/pytest-of-root/pytest-0/test_mark_on_pseudo_function0\ncollected 1 item\n\ntest_mark_on_pseudo_function.py .                                        [100%]\n\n============================== 1 passed in 0.00s ===============================\n_________ test_strict_prohibits_unregistered_markers[--strict-markers] _________\n----------------------------- Captured stdout call -----------------------------\n============================= test session starts ==============================\nplatform linux -- Python 3.9.20, pytest-5.4.1.dev522+g7b77fc086.d20220101, py-1.11.0, pluggy-0.13.1\nrootdir: /tmp/pytest-of-root/pytest-0/test_strict_prohibits_unregistered_markers0\ncollected 0 items / 1 error\n\n==================================== ERRORS ====================================\n________ ERROR collecting test_strict_prohibits_unregistered_markers.py ________\n'unregisteredmark' not found in `markers` configuration option\n=========================== short test summary info ============================\nERROR test_strict_prohibits_unregistered_markers.py\n!!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!\n=============================== 1 error in 0.05s ===============================\n_____________ test_strict_prohibits_unregistered_markers[--strict] _____________\n----------------------------- Captured stdout call -----------------------------\n============================= test session starts ==============================\nplatform linux -- Python 3.9.20, pytest-5.4.1.dev522+g7b77fc086.d20220101, py-1.11.0, pluggy-0.13.1\nrootdir: /tmp/pytest-of-root/pytest-0/test_strict_prohibits_unregistered_markers1\ncollected 0 items / 1 error\n\n==================================== ERRORS ====================================\n________ ERROR collecting test_strict_prohibits_unregistered_markers.py ________\n'unregisteredmark' not found in `markers` configuration option\n=========================== short test summary info ============================\nERROR test_strict_prohibits_unregistered_markers.py\n!!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!\n=============================== 1 error in 0.05s ===============================\n____________________ test_mark_option[xyz-expected_passed0] ____________________\n----------------------------- Captured stdout call -----------------------------\n============================= test session starts ==============================\nplatform linux -- Python 3.9.20, pytest-5.4.1.dev522+g7b77fc086.d20220101, py-1.11.0, pluggy-0.13.1\nrootdir: /tmp/pytest-of-root/pytest-0/test_mark_option0\ncollected 2 items / 1 deselected / 1 selected\n\ntest_mark_option.py .                                                    [100%]\n\n======================= 1 passed, 1 deselected in 0.00s ========================\n_______________ test_mark_option[(((  xyz))  )-expected_passed1] _______________\n----------------------------- Captured stdout call -----------------------------\n============================= test session starts ==============================\nplatform linux -- Python 3.9.20, pytest-5.4.1.dev522+g7b77fc086.d20220101, py-1.11.0, pluggy-0.13.1\nrootdir: /tmp/pytest-of-root/pytest-0/test_mark_option1\ncollected 2 items / 1 deselected / 1 selected\n\ntest_mark_option.py .                                                    [100%]\n\n======================= 1 passed, 1 deselected in 0.00s ========================\n________________ test_mark_option[not not xyz-expected_passed2] ________________\n----------------------------- Captured stdout call -----------------------------\n============================= test session starts ==============================\nplatform linux -- Python 3.9.20, pytest-5.4.1.dev522+g7b77fc086.d20220101, py-1.11.0, pluggy-0.13.1\nrootdir: /tmp/pytest-of-root/pytest-0/test_mark_option2\ncollected 2 items / 1 deselected / 1 selected\n\ntest_mark_option.py .                                                    [100%]\n\n======================= 1 passed, 1 deselected in 0.00s ========================\n_______________ test_mark_option[xyz and xyz2-expected_passed3] ________________\n----------------------------- Captured stdout call -----------------------------\n============================= test session starts ==============================\nplatform linux -- Python 3.9.20, pytest-5.4.1.dev522+g7b77fc086.d20220101, py-1.11.0, pluggy-0.13.1\nrootdir: /tmp/pytest-of-root/pytest-0/test_mark_option3\ncollected 2 items / 2 deselected\n\n============================ 2 deselected in 0.00s =============================\n___________________ test_mark_option[xyz2-expected_passed4] ____________________\n----------------------------- Captured stdout call -----------------------------\n============================= test session starts ==============================\nplatform linux -- Python 3.9.20, pytest-5.4.1.dev522+g7b77fc086.d20220101, py-1.11.0, pluggy-0.13.1\nrootdir: /tmp/pytest-of-root/pytest-0/test_mark_option4\ncollected 2 items / 1 deselected / 1 selected\n\ntest_mark_option.py .                                                    [100%]\n\n======================= 1 passed, 1 deselected in 0.00s ========================\n________________ test_mark_option[xyz or xyz2-expected_passed5] ________________\n----------------------------- Captured stdout call -----------------------------\n============================= test session starts ==============================\nplatform linux -- Python 3.9.20, pytest-5.4.1.dev522+g7b77fc086.d20220101, py-1.11.0, pluggy-0.13.1\nrootdir: /tmp/pytest-of-root/pytest-0/test_mark_option5\ncollected 2 items\n\ntest_mark_option.py ..                                                   [100%]\n\n============================== 2 passed in 0.01s ===============================\n_____________ test_mark_option_custom[interface-expected_passed0] ______________\n----------------------------- Captured stdout call -----------------------------\n============================= test session starts ==============================\nplatform linux -- Python 3.9.20, pytest-5.4.1.dev522+g7b77fc086.d20220101, py-1.11.0, pluggy-0.13.1\nrootdir: /tmp/pytest-of-root/pytest-0/test_mark_option_custom0\ncollected 2 items / 1 deselected / 1 selected\n\ntest_mark_option_custom.py .                                             [100%]\n\n======================= 1 passed, 1 deselected in 0.00s ========================\n___________ test_mark_option_custom[not interface-expected_passed1] ____________\n----------------------------- Captured stdout call -----------------------------\n============================= test session starts ==============================\nplatform linux -- Python 3.9.20, pytest-5.4.1.dev522+g7b77fc086.d20220101, py-1.11.0, pluggy-0.13.1\nrootdir: /tmp/pytest-of-root/pytest-0/test_mark_option_custom1\ncollected 2 items / 1 deselected / 1 selected\n\ntest_mark_option_custom.py .                                             [100%]\n\n======================= 1 passed, 1 deselected in 0.00s ========================\n____________ test_keyword_option_custom[interface-expected_passed0] ____________\n----------------------------- Captured stdout call -----------------------------\n============================= test session starts ==============================\nplatform linux -- Python 3.9.20, pytest-5.4.1.dev522+g7b77fc086.d20220101, py-1.11.0, pluggy-0.13.1\nrootdir: /tmp/pytest-of-root/pytest-0/test_keyword_option_custom0\ncollected 5 items / 4 deselected / 1 selected\n\ntest_keyword_option_custom.py .                                          [100%]\n\n======================= 1 passed, 4 deselected in 0.00s ========================\n__________ test_keyword_option_custom[not interface-expected_passed1] __________\n----------------------------- Captured stdout call -----------------------------\n============================= test session starts ==============================\nplatform linux -- Python 3.9.20, pytest-5.4.1.dev522+g7b77fc086.d20220101, py-1.11.0, pluggy-0.13.1\nrootdir: /tmp/pytest-of-root/pytest-0/test_keyword_option_custom1\ncollected 5 items / 1 deselected / 4 selected\n\ntest_keyword_option_custom.py ....                                       [100%]\n\n======================= 4 passed, 1 deselected in 0.01s ========================\n______________ test_keyword_option_custom[pass-expected_passed2] _______________\n----------------------------- Captured stdout call -----------------------------\n============================= test session starts ==============================\nplatform linux -- Python 3.9.20, pytest-5.4.1.dev522+g7b77fc086.d20220101, py-1.11.0, pluggy-0.13.1\nrootdir: /tmp/pytest-of-root/pytest-0/test_keyword_option_custom2\ncollected 5 items / 4 deselected / 1 selected\n\ntest_keyword_option_custom.py .                                          [100%]\n\n======================= 1 passed, 4 deselected in 0.01s ========================\n____________ test_keyword_option_custom[not pass-expected_passed3] _____________\n----------------------------- Captured stdout call -----------------------------\n============================= test session starts ==============================\nplatform linux -- Python 3.9.20, pytest-5.4.1.dev522+g7b77fc086.d20220101, py-1.11.0, pluggy-0.13.1\nrootdir: /tmp/pytest-of-root/pytest-0/test_keyword_option_custom3\ncollected 5 items / 1 deselected / 4 selected\n\ntest_keyword_option_custom.py ....                                       [100%]\n\n======================= 4 passed, 1 deselected in 0.01s ========================\n_______ test_keyword_option_custom[not not not (pass)-expected_passed4] ________\n----------------------------- Captured stdout call -----------------------------\n============================= test session starts ==============================\nplatform linux -- Python 3.9.20, pytest-5.4.1.dev522+g7b77fc086.d20220101, py-1.11.0, pluggy-0.13.1\nrootdir: /tmp/pytest-of-root/pytest-0/test_keyword_option_custom4\ncollected 5 items / 1 deselected / 4 selected\n\ntest_keyword_option_custom.py ....                                       [100%]\n\n======================= 4 passed, 1 deselected in 0.01s ========================\n_____________ test_keyword_option_custom[1 or 2-expected_passed5] ______________\n----------------------------- Captured stdout call -----------------------------\n============================= test session starts ==============================\nplatform linux -- Python 3.9.20, pytest-5.4.1.dev522+g7b77fc086.d20220101, py-1.11.0, pluggy-0.13.1\nrootdir: /tmp/pytest-of-root/pytest-0/test_keyword_option_custom5\ncollected 5 items / 3 deselected / 2 selected\n\ntest_keyword_option_custom.py ..                                         [100%]\n\n======================= 2 passed, 3 deselected in 0.01s ========================\n__________ test_keyword_option_custom[not (1 or 2)-expected_passed6] ___________\n----------------------------- Captured stdout call -----------------------------\n============================= test session starts ==============================\nplatform linux -- Python 3.9.20, pytest-5.4.1.dev522+g7b77fc086.d20220101, py-1.11.0, pluggy-0.13.1\nrootdir: /tmp/pytest-of-root/pytest-0/test_keyword_option_custom6\ncollected 5 items / 2 deselected / 3 selected\n\ntest_keyword_option_custom.py ...                                        [100%]\n\n======================= 3 passed, 2 deselected in 0.01s ========================\n______________________ test_keyword_option_considers_mark ______________________\n----------------------------- Captured stdout call -----------------------------\n============================= test session starts ==============================\nplatform linux -- Python 3.9.20, pytest-5.4.1.dev522+g7b77fc086.d20220101, py-1.11.0, pluggy-0.13.1\nrootdir: /tmp/pytest-of-root/pytest-0/test_keyword_option_considers_mark0\ncollected 1 item\n\ntest_marks_as_keywords.py .                                              [100%]\n\n============================== 1 passed in 0.00s ===============================\n____________ test_keyword_option_parametrize[None-expected_passed0] ____________\n----------------------------- Captured stdout call -----------------------------\n============================= test session starts ==============================\nplatform linux -- Python 3.9.20, pytest-5.4.1.dev522+g7b77fc086.d20220101, py-1.11.0, pluggy-0.13.1\nrootdir: /tmp/pytest-of-root/pytest-0/test_keyword_option_parametrize0\ncollected 3 items / 2 deselected / 1 selected\n\ntest_keyword_option_parametrize.py .                                     [100%]\n\n======================= 1 passed, 2 deselected in 0.00s ========================\n___________ test_keyword_option_parametrize[[1.3]-expected_passed1] ____________\n----------------------------- Captured stdout call -----------------------------\n============================= test session starts ==============================\nplatform linux -- Python 3.9.20, pytest-5.4.1.dev522+g7b77fc086.d20220101, py-1.11.0, pluggy-0.13.1\nrootdir: /tmp/pytest-of-root/pytest-0/test_keyword_option_parametrize1\ncollected 3 items / 2 deselected / 1 selected\n\ntest_keyword_option_parametrize.py .                                     [100%]\n\n======================= 1 passed, 2 deselected in 0.00s ========================\n____________ test_keyword_option_parametrize[2-3-expected_passed2] _____________\n----------------------------- Captured stdout call -----------------------------\n============================= test session starts ==============================\nplatform linux -- Python 3.9.20, pytest-5.4.1.dev522+g7b77fc086.d20220101, py-1.11.0, pluggy-0.13.1\nrootdir: /tmp/pytest-of-root/pytest-0/test_keyword_option_parametrize2\ncollected 3 items / 2 deselected / 1 selected\n\ntest_keyword_option_parametrize.py .                                     [100%]\n\n======================= 1 passed, 2 deselected in 0.00s ========================\n_________________________ test_parametrize_with_module _________________________\n----------------------------- Captured stdout call -----------------------------\n============================= test session starts ==============================\nplatform linux -- Python 3.9.20, pytest-5.4.1.dev522+g7b77fc086.d20220101, py-1.11.0, pluggy-0.13.1\nrootdir: /tmp/pytest-of-root/pytest-0/test_parametrize_with_module0\ncollected 1 item\n\ntest_parametrize_with_module.py .                                        [100%]\n\n============================== 1 passed in 0.00s ===============================\n________________ test_parametrized_collected_from_command_line _________________\n----------------------------- Captured stdout call -----------------------------\n============================= test session starts ==============================\nplatform linux -- Python 3.9.20, pytest-5.4.1.dev522+g7b77fc086.d20220101, py-1.11.0, pluggy-0.13.1\nrootdir: /tmp/pytest-of-root/pytest-0/test_parametrized_collected_from_command_line0\ncollected 3 items\n\ntest_parametrized_collected_from_command_line.py ...                     [100%]\n\n============================== 3 passed in 0.01s ===============================\n__________________ test_parametrized_collect_with_wrong_args ___________________\n----------------------------- Captured stdout call -----------------------------\n============================= test session starts ==============================\nplatform linux -- Python 3.9.20, pytest-5.4.1.dev522+g7b77fc086.d20220101, py-1.11.0, pluggy-0.13.1\nrootdir: /tmp/pytest-of-root/pytest-0/test_parametrized_collect_with_wrong_args0\ncollected 0 items / 1 error\n\n==================================== ERRORS ====================================\n________ ERROR collecting test_parametrized_collect_with_wrong_args.py _________\ntest_parametrized_collect_with_wrong_args.py::test_func: in \"parametrize\" the number of names (2):\n  ['foo', 'bar']\nmust be equal to the number of values (3):\n  (1, 2, 3)\n=========================== short test summary info ============================\nERROR test_parametrized_collect_with_wrong_args.py\n!!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!\n=============================== 1 error in 0.04s ===============================\n________________________ test_parametrized_with_kwargs _________________________\n----------------------------- Captured stdout call -----------------------------\n============================= test session starts ==============================\nplatform linux -- Python 3.9.20, pytest-5.4.1.dev522+g7b77fc086.d20220101, py-1.11.0, pluggy-0.13.1\nrootdir: /tmp/pytest-of-root/pytest-0/test_parametrized_with_kwargs0\ncollected 4 items\n\ntest_parametrized_with_kwargs.py ....                                    [100%]\n\n============================== 4 passed in 0.01s ===============================\n__________________________ test_parametrize_iterator ___________________________\n----------------------------- Captured stdout call -----------------------------\n============================= test session starts ==============================\nplatform linux -- Python 3.9.20, pytest-5.4.1.dev522+g7b77fc086.d20220101, py-1.11.0, pluggy-0.13.1\nrootdir: /tmp/pytest-of-root/pytest-0/test_parametrize_iterator0\ncollected 3 items\n\ntest_parametrize_iterator.py ...                                         [100%]\n\n============================== 3 passed in 0.01s ===============================\n___________________ TestFunctional.test_merging_markers_deep ___________________\n----------------------------- Captured stdout call -----------------------------\n============================= test session starts ==============================\nplatform linux -- Python 3.9.20, pytest-5.4.1.dev522+g7b77fc086.d20220101, py-1.11.0, pluggy-0.13.1\nrootdir: /tmp/pytest-of-root/pytest-0/test_merging_markers_deep0\ncollected 2 items\n\n<Module test_merging_markers_deep.py>\n  <Class TestA>\n      <Function test_b>\n      <Class TestC>\n          <Function test_d>\n\n============================ no tests ran in 0.00s =============================\n<Function test_b> <NodeKeywords for node <Function test_b>>\n<Function test_d> <NodeKeywords for node <Function test_d>>\n____ TestFunctional.test_mark_decorator_subclass_does_not_propagate_to_base ____\n----------------------------- Captured stdout call -----------------------------\n============================= test session starts ==============================\nplatform linux -- Python 3.9.20, pytest-5.4.1.dev522+g7b77fc086.d20220101, py-1.11.0, pluggy-0.13.1\nrootdir: /tmp/pytest-of-root/pytest-0/test_mark_decorator_subclass_does_not_propagate_to_base0\ncollected 2 items\n\n<Module test_mark_decorator_subclass_does_not_propagate_to_base.py>\n  <Class Test1>\n      <Function test_foo>\n  <Class Test2>\n      <Function test_bar>\n\n============================ no tests ran in 0.00s =============================\n__________ TestFunctional.test_mark_should_not_pass_to_siebling_class __________\n----------------------------- Captured stdout call -----------------------------\n============================= test session starts ==============================\nplatform linux -- Python 3.9.20, pytest-5.4.1.dev522+g7b77fc086.d20220101, py-1.11.0, pluggy-0.13.1\nrootdir: /tmp/pytest-of-root/pytest-0/test_mark_should_not_pass_to_siebling_class0\ncollected 3 items\n\n<Module test_mark_should_not_pass_to_siebling_class.py>\n  <Class TestBase>\n      <Function test_foo>\n  <Class TestSub>\n      <Function test_foo>\n  <Class TestOtherSub>\n      <Function test_foo>\n\n============================ no tests ran in 0.01s =============================\n[<Function test_foo>, <Function test_foo>, <Function test_foo>] ['test_mark_should_not_pass_to_siebling_class.py::TestBase::test_foo', 'test_mark_should_not_pass_to_siebling_class.py::TestSub::test_foo', 'test_mark_should_not_pass_to_siebling_class.py::TestOtherSub::test_foo']\n____________ TestFunctional.test_mark_decorator_baseclasses_merged _____________\n----------------------------- Captured stdout call -----------------------------\n============================= test session starts ==============================\nplatform linux -- Python 3.9.20, pytest-5.4.1.dev522+g7b77fc086.d20220101, py-1.11.0, pluggy-0.13.1\nrootdir: /tmp/pytest-of-root/pytest-0/test_mark_decorator_baseclasses_merged0\ncollected 2 items\n\n<Module test_mark_decorator_baseclasses_merged.py>\n  <Class Test1>\n      <Function test_foo>\n  <Class Test2>\n      <Function test_bar>\n\n============================ no tests ran in 0.01s =============================\n_______________________ TestFunctional.test_mark_closest _______________________\n----------------------------- Captured stdout call -----------------------------\n============================= test session starts ==============================\nplatform linux -- Python 3.9.20, pytest-5.4.1.dev522+g7b77fc086.d20220101, py-1.11.0, pluggy-0.13.1\nrootdir: /tmp/pytest-of-root/pytest-0/test_mark_closest0\ncollected 2 items\n\n<Module test_mark_closest.py>\n  <Class Test>\n      <Function test_has_own>\n      <Function test_has_inherited>\n\n============================ no tests ran in 0.00s =============================\n__________________ TestFunctional.test_mark_with_wrong_marker __________________\n----------------------------- Captured stdout call -----------------------------\n============================= test session starts ==============================\nplatform linux -- Python 3.9.20, pytest-5.4.1.dev522+g7b77fc086.d20220101, py-1.11.0, pluggy-0.13.1\nrootdir: /tmp/pytest-of-root/pytest-0/test_mark_with_wrong_marker0\ncollected 0 items / 1 error\n\n==================================== ERRORS ====================================\n_______________ ERROR collecting test_mark_with_wrong_marker.py ________________\n/testbed/src/_pytest/runner.py:286: in from_call\n    result = func()  # type: Optional[_T]\n/testbed/src/_pytest/runner.py:316: in <lambda>\n    call = CallInfo.from_call(lambda: list(collector.collect()), \"collect\")\n/testbed/src/_pytest/python.py:488: in collect\n    self._inject_setup_module_fixture()\n/testbed/src/_pytest/python.py:501: in _inject_setup_module_fixture\n    self.obj, (\"setUpModule\", \"setup_module\")\n/testbed/src/_pytest/python.py:293: in obj\n    self.own_markers.extend(get_unpacked_marks(self.obj))\n/testbed/src/_pytest/mark/structures.py:352: in get_unpacked_marks\n    return normalize_mark_list(mark_list)\n/testbed/src/_pytest/mark/structures.py:367: in normalize_mark_list\n    raise TypeError(\"got {!r} instead of Mark\".format(mark))\nE   TypeError: got <class 'test_mark_with_wrong_marker.pytestmark'> instead of Mark\n=========================== short test summary info ============================\nERROR test_mark_with_wrong_marker.py - TypeError: got <class 'test_mark_with_...\n!!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!\n=============================== 1 error in 0.08s ===============================\n_______________ TestFunctional.test_mark_dynamically_in_funcarg ________________\n----------------------------- Captured stdout call -----------------------------\n============================= test session starts ==============================\nplatform linux -- Python 3.9.20, pytest-5.4.1.dev522+g7b77fc086.d20220101, py-1.11.0, pluggy-0.13.1\nrootdir: /tmp/pytest-of-root/pytest-0/test_mark_dynamically_in_funcarg0\ncollected 1 item\n\ntest_mark_dynamically_in_funcarg.py .                                    [100%]\nkeyword: {'test_func': 1, 'hello': 1, 'test_mark_dynamically_in_funcarg.py': 1, 'test_mark_dynamically_in_funcarg0': 1}\n\n============================== 1 passed in 0.00s ===============================\n____________ TestFunctional.test_no_marker_match_on_unmarked_names _____________\n----------------------------- Captured stdout call -----------------------------\n============================= test session starts ==============================\nplatform linux -- Python 3.9.20, pytest-5.4.1.dev522+g7b77fc086.d20220101, py-1.11.0, pluggy-0.13.1\nrootdir: /tmp/pytest-of-root/pytest-0/test_no_marker_match_on_unmarked_names0\ncollected 2 items / 2 deselected\n\n============================ 2 deselected in 0.00s =============================\n__________________ TestFunctional.test_keywords_at_node_level __________________\n----------------------------- Captured stdout call -----------------------------\n============================= test session starts ==============================\nplatform linux -- Python 3.9.20, pytest-5.4.1.dev522+g7b77fc086.d20220101, py-1.11.0, pluggy-0.13.1\nrootdir: /tmp/pytest-of-root/pytest-0/test_keywords_at_node_level0\ncollected 1 item\n\ntest_keywords_at_node_level.py .                                         [100%]\n\n============================== 1 passed in 0.01s ===============================\n________________ TestFunctional.test_keyword_added_for_session _________________\n----------------------------- Captured stdout call -----------------------------\n============================= test session starts ==============================\nplatform linux -- Python 3.9.20, pytest-5.4.1.dev522+g7b77fc086.d20220101, py-1.11.0, pluggy-0.13.1\nrootdir: /tmp/pytest-of-root/pytest-0/test_keyword_added_for_session0\ncollected 1 item\n\ntest_keyword_added_for_session.py .                                      [100%]\n\n============================== 1 passed in 0.01s ===============================\n___________________ TestFunctional.test_mark_from_parameters ___________________\n----------------------------- Captured stdout call -----------------------------\n============================= test session starts ==============================\nplatform linux -- Python 3.9.20, pytest-5.4.1.dev522+g7b77fc086.d20220101, py-1.11.0, pluggy-0.13.1\nrootdir: /tmp/pytest-of-root/pytest-0/test_mark_from_parameters0\ncollected 1 item\n\ntest_mark_from_parameters.py s                                           [100%]\n\n============================== 1 skipped in 0.00s ==============================\n_________________ TestFunctional.test_reevaluate_dynamic_expr __________________\n----------------------------- Captured stdout call -----------------------------\n============================= test session starts ==============================\nplatform linux -- Python 3.9.20, pytest-5.4.1.dev522+g7b77fc086.d20220101, py-1.11.0, pluggy-0.13.1\nrootdir: /tmp/pytest-of-root/pytest-0/test_reevaluate_dynamic_expr0\ncollected 2 items\n\ntest_reevaluate_dynamic_expr1.py s                                       [ 50%]\ntest_reevaluate_dynamic_expr2.py .                                       [100%]\n\n========================= 1 passed, 1 skipped in 0.01s =========================\n___________________ TestKeywordSelection.test_select_simple ____________________\n----------------------------- Captured stdout call -----------------------------\n============================= test session starts ==============================\nplatform linux -- Python 3.9.20, pytest-5.4.1.dev522+g7b77fc086.d20220101, py-1.11.0, pluggy-0.13.1\nrootdir: /tmp/pytest-of-root/pytest-0/test_select_simple0\ncollected 2 items / 1 deselected / 1 selected\n\ntest_select_simple.py F\n\n=================================== FAILURES ===================================\n___________________________________ test_one ___________________________________\n\n    def test_one():\n>       assert 0\nE       assert 0\n\ntest_select_simple.py:2: AssertionError\n=========================== short test summary info ============================\nFAILED test_select_simple.py::test_one - assert 0\n======================= 1 failed, 1 deselected in 0.01s ========================\n============================= test session starts ==============================\nplatform linux -- Python 3.9.20, pytest-5.4.1.dev522+g7b77fc086.d20220101, py-1.11.0, pluggy-0.13.1\nrootdir: /tmp/pytest-of-root/pytest-0/test_select_simple0\ncollected 2 items / 1 deselected / 1 selected\n\ntest_select_simple.py F\n\n=================================== FAILURES ===================================\n___________________________________ test_one ___________________________________\n\n    def test_one():\n>       assert 0\nE       assert 0\n\ntest_select_simple.py:2: AssertionError\n=========================== short test summary info ============================\nFAILED test_select_simple.py::test_one - assert 0\n======================= 1 failed, 1 deselected in 0.01s ========================\n============================= test session starts ==============================\nplatform linux -- Python 3.9.20, pytest-5.4.1.dev522+g7b77fc086.d20220101, py-1.11.0, pluggy-0.13.1\nrootdir: /tmp/pytest-of-root/pytest-0/test_select_simple0\ncollected 2 items / 1 deselected / 1 selected\n\ntest_select_simple.py F\n\n=================================== FAILURES ===================================\n__________________________ TestClass.test_method_one ___________________________\n\nself = <test_select_simple.TestClass object at 0x7fcfe5787640>\n\n    def test_method_one(self):\n>       assert 42 == 43\nE       assert 42 == 43\n\ntest_select_simple.py:5: AssertionError\n=========================== short test summary info ============================\nFAILED test_select_simple.py::TestClass::test_method_one - assert 42 == 43\n======================= 1 failed, 1 deselected in 0.01s ========================\n_____________ TestKeywordSelection.test_select_extra_keywords[xxx] _____________\n----------------------------- Captured stdout call -----------------------------\n============================= test session starts ==============================\nplatform linux -- Python 3.9.20, pytest-5.4.1.dev522+g7b77fc086.d20220101, py-1.11.0, pluggy-0.13.1\nrootdir: /tmp/pytest-of-root/pytest-0/test_select_extra_keywords0\ncollected 2 items / 1 deselected / 1 selected\n\ntest_select.py .\n\n======================= 1 passed, 1 deselected in 0.01s ========================\nkeyword 'xxx'\n_______ TestKeywordSelection.test_select_extra_keywords[xxx and test_2] ________\n----------------------------- Captured stdout call -----------------------------\n============================= test session starts ==============================\nplatform linux -- Python 3.9.20, pytest-5.4.1.dev522+g7b77fc086.d20220101, py-1.11.0, pluggy-0.13.1\nrootdir: /tmp/pytest-of-root/pytest-0/test_select_extra_keywords1\ncollected 2 items / 1 deselected / 1 selected\n\ntest_select.py .\n\n======================= 1 passed, 1 deselected in 0.01s ========================\nkeyword 'xxx and test_2'\n__________ TestKeywordSelection.test_select_extra_keywords[TestClass] __________\n----------------------------- Captured stdout call -----------------------------\n============================= test session starts ==============================\nplatform linux -- Python 3.9.20, pytest-5.4.1.dev522+g7b77fc086.d20220101, py-1.11.0, pluggy-0.13.1\nrootdir: /tmp/pytest-of-root/pytest-0/test_select_extra_keywords2\ncollected 2 items / 1 deselected / 1 selected\n\ntest_select.py .\n\n======================= 1 passed, 1 deselected in 0.00s ========================\nkeyword 'TestClass'\n_____ TestKeywordSelection.test_select_extra_keywords[xxx and not test_1] ______\n----------------------------- Captured stdout call -----------------------------\n============================= test session starts ==============================\nplatform linux -- Python 3.9.20, pytest-5.4.1.dev522+g7b77fc086.d20220101, py-1.11.0, pluggy-0.13.1\nrootdir: /tmp/pytest-of-root/pytest-0/test_select_extra_keywords3\ncollected 2 items / 1 deselected / 1 selected\n\ntest_select.py .\n\n======================= 1 passed, 1 deselected in 0.01s ========================\nkeyword 'xxx and not test_1'\n____ TestKeywordSelection.test_select_extra_keywords[TestClass and test_2] _____\n----------------------------- Captured stdout call -----------------------------\n============================= test session starts ==============================\nplatform linux -- Python 3.9.20, pytest-5.4.1.dev522+g7b77fc086.d20220101, py-1.11.0, pluggy-0.13.1\nrootdir: /tmp/pytest-of-root/pytest-0/test_select_extra_keywords4\ncollected 2 items / 1 deselected / 1 selected\n\ntest_select.py .\n\n======================= 1 passed, 1 deselected in 0.01s ========================\nkeyword 'TestClass and test_2'\n_ TestKeywordSelection.test_select_extra_keywords[xxx and TestClass and test_2] _\n----------------------------- Captured stdout call -----------------------------\n============================= test session starts ==============================\nplatform linux -- Python 3.9.20, pytest-5.4.1.dev522+g7b77fc086.d20220101, py-1.11.0, pluggy-0.13.1\nrootdir: /tmp/pytest-of-root/pytest-0/test_select_extra_keywords5\ncollected 2 items / 1 deselected / 1 selected\n\ntest_select.py .\n\n======================= 1 passed, 1 deselected in 0.00s ========================\nkeyword 'xxx and TestClass and test_2'\n___________________ TestKeywordSelection.test_select_starton ___________________\n----------------------------- Captured stdout call -----------------------------\n============================= test session starts ==============================\nplatform linux -- Python 3.9.20, pytest-5.4.1.dev522+g7b77fc086.d20220101, py-1.11.0, pluggy-0.13.1\nrootdir: /tmp/pytest-of-root/pytest-0/test_select_starton0\ncollected 3 items / 1 deselected / 2 selected\n\ntest_threepass.py ..                                                     [100%]\n\n=============================== warnings summary ===============================\n/testbed/src/_pytest/mark/__init__.py:259\n  /testbed/src/_pytest/mark/__init__.py:259: PytestDeprecationWarning: The `-k 'expr:'` syntax to -k is deprecated.\n  Please open an issue if you use this and want a replacement.\n    deselect_by_keyword(items, config)\n\n-- Docs: https://docs.pytest.org/en/latest/warnings.html\n================== 2 passed, 1 deselected, 1 warning in 0.01s ==================\n___________________ TestKeywordSelection.test_keyword_extra ____________________\n----------------------------- Captured stdout call -----------------------------\n============================= test session starts ==============================\nplatform linux -- Python 3.9.20, pytest-5.4.1.dev522+g7b77fc086.d20220101, py-1.11.0, pluggy-0.13.1\nrootdir: /tmp/pytest-of-root/pytest-0/test_keyword_extra0\ncollected 1 item\n\ntest_keyword_extra.py F                                                  [100%]\n\n=================================== FAILURES ===================================\n___________________________________ test_one ___________________________________\n\n    def test_one():\n>       assert 0\nE       assert 0\n\ntest_keyword_extra.py:2: AssertionError\n=========================== short test summary info ============================\nFAILED test_keyword_extra.py::test_one - assert 0\n============================== 1 failed in 0.01s ===============================\n________________ TestKeywordSelection.test_no_magic_values[__] _________________\n----------------------------- Captured stdout call -----------------------------\n============================= test session starts ==============================\nplatform linux -- Python 3.9.20, pytest-5.4.1.dev522+g7b77fc086.d20220101, py-1.11.0, pluggy-0.13.1\nrootdir: /tmp/pytest-of-root/pytest-0/test_no_magic_values0\ncollected 1 item / 1 deselected\n\n============================ 1 deselected in 0.00s =============================\n_________________ TestKeywordSelection.test_no_magic_values[+] _________________\n----------------------------- Captured stdout call -----------------------------\n============================= test session starts ==============================\nplatform linux -- Python 3.9.20, pytest-5.4.1.dev522+g7b77fc086.d20220101, py-1.11.0, pluggy-0.13.1\nrootdir: /tmp/pytest-of-root/pytest-0/test_no_magic_values1\ncollected 1 item / 1 deselected\n\n============================ 1 deselected in 0.00s =============================\n________________ TestKeywordSelection.test_no_magic_values[..] _________________\n----------------------------- Captured stdout call -----------------------------\n============================= test session starts ==============================\nplatform linux -- Python 3.9.20, pytest-5.4.1.dev522+g7b77fc086.d20220101, py-1.11.0, pluggy-0.13.1\nrootdir: /tmp/pytest-of-root/pytest-0/test_no_magic_values2\ncollected 1 item / 1 deselected\n\n============================ 1 deselected in 0.00s =============================\n_______ TestKeywordSelection.test_no_match_directories_outside_the_suite _______\n----------------------------- Captured stdout call -----------------------------\n============================= test session starts ==============================\nplatform linux -- Python 3.9.20, pytest-5.4.1.dev522+g7b77fc086.d20220101, py-1.11.0, pluggy-0.13.1\nrootdir: /tmp/pytest-of-root/pytest-0/test_no_match_directories_outside_the_suite0\ncollected 2 items\n\n<Package tests>\n  <Module test_foo.py>\n    <Function test_aaa>\n    <Function test_ddd>\n\n============================ no tests ran in 0.00s =============================\n============================= test session starts ==============================\nplatform linux -- Python 3.9.20, pytest-5.4.1.dev522+g7b77fc086.d20220101, py-1.11.0, pluggy-0.13.1\nrootdir: /tmp/pytest-of-root/pytest-0/test_no_match_directories_outside_the_suite0\ncollected 2 items / 2 deselected\n\n============================ 2 deselected in 0.00s =============================\n============================= test session starts ==============================\nplatform linux -- Python 3.9.20, pytest-5.4.1.dev522+g7b77fc086.d20220101, py-1.11.0, pluggy-0.13.1\nrootdir: /tmp/pytest-of-root/pytest-0/test_no_match_directories_outside_the_suite0\ncollected 2 items / 1 deselected / 1 selected\n\n<Package tests>\n  <Module test_foo.py>\n    <Function test_ddd>\n\n============================ 1 deselected in 0.00s =============================\n____________________ test_parameterset_for_fail_at_collect _____________________\n----------------------------- Captured stdout call -----------------------------\n============================= test session starts ==============================\nplatform linux -- Python 3.9.20, pytest-5.4.1.dev522+g7b77fc086.d20220101, py-1.11.0, pluggy-0.13.1\nrootdir: /tmp/pytest-of-root/pytest-0/test_parameterset_for_fail_at_collect0, configfile: tox.ini\ncollected 0 items / 1 error\n\n==================================== ERRORS ====================================\n__________ ERROR collecting test_parameterset_for_fail_at_collect.py ___________\nEmpty parameter set in 'test' at line 3\n=========================== short test summary info ============================\nERROR test_parameterset_for_fail_at_collect.py\n!!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!\n=============================== 1 error in 0.04s ===============================\n________________________ test_mark_expressions_no_smear ________________________\n----------------------------- Captured stdout call -----------------------------\n============================= test session starts ==============================\nplatform linux -- Python 3.9.20, pytest-5.4.1.dev522+g7b77fc086.d20220101, py-1.11.0, pluggy-0.13.1\nrootdir: /tmp/pytest-of-root/pytest-0/test_mark_expressions_no_smear0\ncollected 2 items / 1 deselected / 1 selected\n\ntest_mark_expressions_no_smear.py .                                      [100%]\n\n======================= 1 passed, 1 deselected in 0.01s ========================\n________________________ test_markers_from_parametrize _________________________\n----------------------------- Captured stdout call -----------------------------\n============================= test session starts ==============================\nplatform linux -- Python 3.9.20, pytest-5.4.1.dev522+g7b77fc086.d20220101, py-1.11.0, pluggy-0.13.1\nrootdir: /tmp/pytest-of-root/pytest-0/test_markers_from_parametrize0\ncollected 4 items\n\ntest_markers_from_parametrize.py ....                                    [100%]\n\n============================== 4 passed in 0.01s ===============================\n___________ test_marker_expr_eval_failure_handling[NOT internal_err] ___________\n----------------------------- Captured stdout call -----------------------------\n============================= test session starts ==============================\nplatform linux -- Python 3.9.20, pytest-5.4.1.dev522+g7b77fc086.d20220101, py-1.11.0, pluggy-0.13.1\nrootdir: /tmp/pytest-of-root/pytest-0/test_marker_expr_eval_failure_handling0\ncollected 1 item\n\n============================ no tests ran in 0.00s =============================\n----------------------------- Captured stderr call -----------------------------\nERROR: Wrong expression passed to '-m': NOT internal_err: at column 5: expected end of input; got identifier\n\n__________ test_marker_expr_eval_failure_handling[NOT (internal_err)] __________\n----------------------------- Captured stdout call -----------------------------\n============================= test session starts ==============================\nplatform linux -- Python 3.9.20, pytest-5.4.1.dev522+g7b77fc086.d20220101, py-1.11.0, pluggy-0.13.1\nrootdir: /tmp/pytest-of-root/pytest-0/test_marker_expr_eval_failure_handling1\ncollected 1 item\n\n============================ no tests ran in 0.00s =============================\n----------------------------- Captured stderr call -----------------------------\nERROR: Wrong expression passed to '-m': NOT (internal_err): at column 5: expected end of input; got left parenthesis\n\n________________ test_marker_expr_eval_failure_handling[bogus/] ________________\n----------------------------- Captured stdout call -----------------------------\n============================= test session starts ==============================\nplatform linux -- Python 3.9.20, pytest-5.4.1.dev522+g7b77fc086.d20220101, py-1.11.0, pluggy-0.13.1\nrootdir: /tmp/pytest-of-root/pytest-0/test_marker_expr_eval_failure_handling2\ncollected 1 item\n\n============================ no tests ran in 0.00s =============================\n----------------------------- Captured stderr call -----------------------------\nERROR: Wrong expression passed to '-m': bogus/: at column 6: unexpected character \"/\"\n\n=========================== short test summary info ============================\nPASSED testing/test_mark.py::TestMark::test_pytest_exists_in_namespace_all[py.test-mark]\nPASSED testing/test_mark.py::TestMark::test_pytest_exists_in_namespace_all[py.test-param]\nPASSED testing/test_mark.py::TestMark::test_pytest_exists_in_namespace_all[pytest-mark]\nPASSED testing/test_mark.py::TestMark::test_pytest_exists_in_namespace_all[pytest-param]\nPASSED testing/test_mark.py::TestMark::test_pytest_mark_notcallable\nPASSED testing/test_mark.py::TestMark::test_mark_with_param\nPASSED testing/test_mark.py::TestMark::test_pytest_mark_name_starts_with_underscore\nPASSED testing/test_mark.py::TestMarkDecorator::test__eq__[lhs0-rhs0-True]\nPASSED testing/test_mark.py::TestMarkDecorator::test__eq__[lhs1-rhs1-False]\nPASSED testing/test_mark.py::TestMarkDecorator::test__eq__[lhs2-bar-False]\nPASSED testing/test_mark.py::TestMarkDecorator::test__eq__[foo-rhs3-False]\nPASSED testing/test_mark.py::TestMarkDecorator::test_aliases\nPASSED testing/test_mark.py::test_addmarker_order\nPASSED testing/test_mark.py::test_pytest_param_id_requires_string\nPASSED testing/test_mark.py::test_pytest_param_id_allows_none_or_string[None]\nPASSED testing/test_mark.py::test_pytest_param_id_allows_none_or_string[hello world]\nPASSED testing/test_mark.py::test_marked_class_run_twice\nPASSED testing/test_mark.py::test_ini_markers\nPASSED testing/test_mark.py::test_markers_option\nPASSED testing/test_mark.py::test_ini_markers_whitespace\nPASSED testing/test_mark.py::test_marker_without_description\nPASSED testing/test_mark.py::test_markers_option_with_plugin_in_current_dir\nPASSED testing/test_mark.py::test_mark_on_pseudo_function\nPASSED testing/test_mark.py::test_strict_prohibits_unregistered_markers[--strict-markers]\nPASSED testing/test_mark.py::test_strict_prohibits_unregistered_markers[--strict]\nPASSED testing/test_mark.py::test_mark_option[xyz-expected_passed0]\nPASSED testing/test_mark.py::test_mark_option[(((  xyz))  )-expected_passed1]\nPASSED testing/test_mark.py::test_mark_option[not not xyz-expected_passed2]\nPASSED testing/test_mark.py::test_mark_option[xyz and xyz2-expected_passed3]\nPASSED testing/test_mark.py::test_mark_option[xyz2-expected_passed4]\nPASSED testing/test_mark.py::test_mark_option[xyz or xyz2-expected_passed5]\nPASSED testing/test_mark.py::test_mark_option_custom[interface-expected_passed0]\nPASSED testing/test_mark.py::test_mark_option_custom[not interface-expected_passed1]\nPASSED testing/test_mark.py::test_keyword_option_custom[interface-expected_passed0]\nPASSED testing/test_mark.py::test_keyword_option_custom[not interface-expected_passed1]\nPASSED testing/test_mark.py::test_keyword_option_custom[pass-expected_passed2]\nPASSED testing/test_mark.py::test_keyword_option_custom[not pass-expected_passed3]\nPASSED testing/test_mark.py::test_keyword_option_custom[not not not (pass)-expected_passed4]\nPASSED testing/test_mark.py::test_keyword_option_custom[1 or 2-expected_passed5]\nPASSED testing/test_mark.py::test_keyword_option_custom[not (1 or 2)-expected_passed6]\nPASSED testing/test_mark.py::test_keyword_option_considers_mark\nPASSED testing/test_mark.py::test_keyword_option_parametrize[None-expected_passed0]\nPASSED testing/test_mark.py::test_keyword_option_parametrize[[1.3]-expected_passed1]\nPASSED testing/test_mark.py::test_keyword_option_parametrize[2-3-expected_passed2]\nPASSED testing/test_mark.py::test_parametrize_with_module\nPASSED testing/test_mark.py::test_keyword_option_wrong_arguments[foo or-at column 7: expected not OR left parenthesis OR identifier; got end of input]\nPASSED testing/test_mark.py::test_keyword_option_wrong_arguments[foo or or-at column 8: expected not OR left parenthesis OR identifier; got or]\nPASSED testing/test_mark.py::test_keyword_option_wrong_arguments[(foo-at column 5: expected right parenthesis; got end of input]\nPASSED testing/test_mark.py::test_keyword_option_wrong_arguments[foo bar-at column 5: expected end of input; got identifier]\nPASSED testing/test_mark.py::test_keyword_option_wrong_arguments[or or-at column 1: expected not OR left parenthesis OR identifier; got or]\nPASSED testing/test_mark.py::test_keyword_option_wrong_arguments[not or-at column 5: expected not OR left parenthesis OR identifier; got or]\nPASSED testing/test_mark.py::test_parametrized_collected_from_command_line\nPASSED testing/test_mark.py::test_parametrized_collect_with_wrong_args\nPASSED testing/test_mark.py::test_parametrized_with_kwargs\nPASSED testing/test_mark.py::test_parametrize_iterator\nPASSED testing/test_mark.py::TestFunctional::test_merging_markers_deep\nPASSED testing/test_mark.py::TestFunctional::test_mark_decorator_subclass_does_not_propagate_to_base\nPASSED testing/test_mark.py::TestFunctional::test_mark_should_not_pass_to_siebling_class\nPASSED testing/test_mark.py::TestFunctional::test_mark_decorator_baseclasses_merged\nPASSED testing/test_mark.py::TestFunctional::test_mark_closest\nPASSED testing/test_mark.py::TestFunctional::test_mark_with_wrong_marker\nPASSED testing/test_mark.py::TestFunctional::test_mark_dynamically_in_funcarg\nPASSED testing/test_mark.py::TestFunctional::test_no_marker_match_on_unmarked_names\nPASSED testing/test_mark.py::TestFunctional::test_keywords_at_node_level\nPASSED testing/test_mark.py::TestFunctional::test_keyword_added_for_session\nPASSED testing/test_mark.py::TestFunctional::test_mark_from_parameters\nPASSED testing/test_mark.py::TestFunctional::test_reevaluate_dynamic_expr\nPASSED testing/test_mark.py::TestKeywordSelection::test_select_simple\nPASSED testing/test_mark.py::TestKeywordSelection::test_select_extra_keywords[xxx]\nPASSED testing/test_mark.py::TestKeywordSelection::test_select_extra_keywords[xxx and test_2]\nPASSED testing/test_mark.py::TestKeywordSelection::test_select_extra_keywords[TestClass]\nPASSED testing/test_mark.py::TestKeywordSelection::test_select_extra_keywords[xxx and not test_1]\nPASSED testing/test_mark.py::TestKeywordSelection::test_select_extra_keywords[TestClass and test_2]\nPASSED testing/test_mark.py::TestKeywordSelection::test_select_extra_keywords[xxx and TestClass and test_2]\nPASSED testing/test_mark.py::TestKeywordSelection::test_select_starton\nPASSED testing/test_mark.py::TestKeywordSelection::test_keyword_extra\nPASSED testing/test_mark.py::TestKeywordSelection::test_no_magic_values[__]\nPASSED testing/test_mark.py::TestKeywordSelection::test_no_magic_values[+]\nPASSED testing/test_mark.py::TestKeywordSelection::test_no_magic_values[..]\nPASSED testing/test_mark.py::TestKeywordSelection::test_no_match_directories_outside_the_suite\nPASSED testing/test_mark.py::test_parameterset_for_parametrize_marks[None]\nPASSED testing/test_mark.py::test_parameterset_for_parametrize_marks[]\nPASSED testing/test_mark.py::test_parameterset_for_parametrize_marks[skip]\nPASSED testing/test_mark.py::test_parameterset_for_parametrize_marks[xfail]\nPASSED testing/test_mark.py::test_parameterset_for_fail_at_collect\nPASSED testing/test_mark.py::test_parameterset_for_parametrize_bad_markname\nPASSED testing/test_mark.py::test_mark_expressions_no_smear\nPASSED testing/test_mark.py::test_markers_from_parametrize\nPASSED testing/test_mark.py::test_marker_expr_eval_failure_handling[NOT internal_err]\nPASSED testing/test_mark.py::test_marker_expr_eval_failure_handling[NOT (internal_err)]\nPASSED testing/test_mark.py::test_marker_expr_eval_failure_handling[bogus/]\nXFAIL testing/test_mark.py::TestKeywordSelection::test_keyword_extra_dash\n======================== 91 passed, 1 xfailed in 1.97s =========================\n", {"test_strict_prohibits_unregistered_markers.py": "ERROR", "test_parametrized_collect_with_wrong_args.py": "ERROR", "test_mark_with_wrong_marker.py": "ERROR", "test_select_simple.py::test_one": "FAILED", "test_select_simple.py::TestClass::test_method_one": "FAILED", "test_keyword_extra.py::test_one": "FAILED", "test_parameterset_for_fail_at_collect.py": "ERROR", "Wrong": "ERROR:", "testing/test_mark.py::TestMark::test_pytest_exists_in_namespace_all[py.test-mark]": "PASSED", "testing/test_mark.py::TestMark::test_pytest_exists_in_namespace_all[py.test-param]": "PASSED", "testing/test_mark.py::TestMark::test_pytest_exists_in_namespace_all[pytest-mark]": "PASSED", "testing/test_mark.py::TestMark::test_pytest_exists_in_namespace_all[pytest-param]": "PASSED", "testing/test_mark.py::TestMark::test_pytest_mark_notcallable": "PASSED", "testing/test_mark.py::TestMark::test_mark_with_param": "PASSED", "testing/test_mark.py::TestMark::test_pytest_mark_name_starts_with_underscore": "PASSED", "testing/test_mark.py::TestMarkDecorator::test__eq__[lhs0-rhs0-True]": "PASSED", "testing/test_mark.py::TestMarkDecorator::test__eq__[lhs1-rhs1-False]": "PASSED", "testing/test_mark.py::TestMarkDecorator::test__eq__[lhs2-bar-False]": "PASSED", "testing/test_mark.py::TestMarkDecorator::test__eq__[foo-rhs3-False]": "PASSED", "testing/test_mark.py::TestMarkDecorator::test_aliases": "PASSED", "testing/test_mark.py::test_addmarker_order": "PASSED", "testing/test_mark.py::test_pytest_param_id_requires_string": "PASSED", "testing/test_mark.py::test_pytest_param_id_allows_none_or_string[None]": "PASSED", "testing/test_mark.py::test_pytest_param_id_allows_none_or_string[hello": "PASSED", "testing/test_mark.py::test_marked_class_run_twice": "PASSED", "testing/test_mark.py::test_ini_markers": "PASSED", "testing/test_mark.py::test_markers_option": "PASSED", "testing/test_mark.py::test_ini_markers_whitespace": "PASSED", "testing/test_mark.py::test_marker_without_description": "PASSED", "testing/test_mark.py::test_markers_option_with_plugin_in_current_dir": "PASSED", "testing/test_mark.py::test_mark_on_pseudo_function": "PASSED", "testing/test_mark.py::test_strict_prohibits_unregistered_markers[--strict-markers]": "PASSED", "testing/test_mark.py::test_strict_prohibits_unregistered_markers[--strict]": "PASSED", "testing/test_mark.py::test_mark_option[xyz-expected_passed0]": "PASSED", "testing/test_mark.py::test_mark_option[(((": "PASSED", "testing/test_mark.py::test_mark_option[not": "PASSED", "testing/test_mark.py::test_mark_option[xyz": "PASSED", "testing/test_mark.py::test_mark_option[xyz2-expected_passed4]": "PASSED", "testing/test_mark.py::test_mark_option_custom[interface-expected_passed0]": "PASSED", "testing/test_mark.py::test_mark_option_custom[not": "PASSED", "testing/test_mark.py::test_keyword_option_custom[interface-expected_passed0]": "PASSED", "testing/test_mark.py::test_keyword_option_custom[not": "PASSED", "testing/test_mark.py::test_keyword_option_custom[pass-expected_passed2]": "PASSED", "testing/test_mark.py::test_keyword_option_custom[1": "PASSED", "testing/test_mark.py::test_keyword_option_considers_mark": "PASSED", "testing/test_mark.py::test_keyword_option_parametrize[None-expected_passed0]": "PASSED", "testing/test_mark.py::test_keyword_option_parametrize[[1.3]-expected_passed1]": "PASSED", "testing/test_mark.py::test_keyword_option_parametrize[2-3-expected_passed2]": "PASSED", "testing/test_mark.py::test_parametrize_with_module": "PASSED", "testing/test_mark.py::test_keyword_option_wrong_arguments[foo": "PASSED", "testing/test_mark.py::test_keyword_option_wrong_arguments[(foo-at": "PASSED", "testing/test_mark.py::test_keyword_option_wrong_arguments[or": "PASSED", "testing/test_mark.py::test_keyword_option_wrong_arguments[not": "PASSED", "testing/test_mark.py::test_parametrized_collected_from_command_line": "PASSED", "testing/test_mark.py::test_parametrized_collect_with_wrong_args": "PASSED", "testing/test_mark.py::test_parametrized_with_kwargs": "PASSED", "testing/test_mark.py::test_parametrize_iterator": "PASSED", "testing/test_mark.py::TestFunctional::test_merging_markers_deep": "PASSED", "testing/test_mark.py::TestFunctional::test_mark_decorator_subclass_does_not_propagate_to_base": "PASSED", "testing/test_mark.py::TestFunctional::test_mark_should_not_pass_to_siebling_class": "PASSED", "testing/test_mark.py::TestFunctional::test_mark_decorator_baseclasses_merged": "PASSED", "testing/test_mark.py::TestFunctional::test_mark_closest": "PASSED", "testing/test_mark.py::TestFunctional::test_mark_with_wrong_marker": "PASSED", "testing/test_mark.py::TestFunctional::test_mark_dynamically_in_funcarg": "PASSED", "testing/test_mark.py::TestFunctional::test_no_marker_match_on_unmarked_names": "PASSED", "testing/test_mark.py::TestFunctional::test_keywords_at_node_level": "PASSED", "testing/test_mark.py::TestFunctional::test_keyword_added_for_session": "PASSED", "testing/test_mark.py::TestFunctional::test_mark_from_parameters": "PASSED", "testing/test_mark.py::TestFunctional::test_reevaluate_dynamic_expr": "PASSED", "testing/test_mark.py::TestKeywordSelection::test_select_simple": "PASSED", "testing/test_mark.py::TestKeywordSelection::test_select_extra_keywords[xxx]": "PASSED", "testing/test_mark.py::TestKeywordSelection::test_select_extra_keywords[xxx": "PASSED", "testing/test_mark.py::TestKeywordSelection::test_select_extra_keywords[TestClass]": "PASSED", "testing/test_mark.py::TestKeywordSelection::test_select_extra_keywords[TestClass": "PASSED", "testing/test_mark.py::TestKeywordSelection::test_select_starton": "PASSED", "testing/test_mark.py::TestKeywordSelection::test_keyword_extra": "PASSED", "testing/test_mark.py::TestKeywordSelection::test_no_magic_values[__]": "PASSED", "testing/test_mark.py::TestKeywordSelection::test_no_magic_values[+]": "PASSED", "testing/test_mark.py::TestKeywordSelection::test_no_magic_values[..]": "PASSED", "testing/test_mark.py::TestKeywordSelection::test_no_match_directories_outside_the_suite": "PASSED", "testing/test_mark.py::test_parameterset_for_parametrize_marks[None]": "PASSED", "testing/test_mark.py::test_parameterset_for_parametrize_marks[]": "PASSED", "testing/test_mark.py::test_parameterset_for_parametrize_marks[skip]": "PASSED", "testing/test_mark.py::test_parameterset_for_parametrize_marks[xfail]": "PASSED", "testing/test_mark.py::test_parameterset_for_fail_at_collect": "PASSED", "testing/test_mark.py::test_parameterset_for_parametrize_bad_markname": "PASSED", "testing/test_mark.py::test_mark_expressions_no_smear": "PASSED", "testing/test_mark.py::test_markers_from_parametrize": "PASSED", "testing/test_mark.py::test_marker_expr_eval_failure_handling[NOT": "PASSED", "testing/test_mark.py::test_marker_expr_eval_failure_handling[bogus/]": "PASSED"}]